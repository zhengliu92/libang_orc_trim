[
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "groundingdino.util.box_ops",
        "description": "groundingdino.util.box_ops",
        "isExtraImport": true,
        "detail": "groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "clean_state_dict",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "clean_state_dict",
        "importPath": "groundingdino.util.misc",
        "description": "groundingdino.util.misc",
        "isExtraImport": true,
        "detail": "groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "IntermediateLayerGetter",
        "importPath": "torchvision.models._utils",
        "description": "torchvision.models._utils",
        "isExtraImport": true,
        "detail": "torchvision.models._utils",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "nms",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "nms",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "box_area",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "BertConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertPreTrainedModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "box_ops",
        "importPath": "groundingdino.util",
        "description": "groundingdino.util",
        "isExtraImport": true,
        "detail": "groundingdino.util",
        "documentation": {}
    },
    {
        "label": "get_tokenlizer",
        "importPath": "groundingdino.util",
        "description": "groundingdino.util",
        "isExtraImport": true,
        "detail": "groundingdino.util",
        "documentation": {}
    },
    {
        "label": "get_phrases_from_posmap",
        "importPath": "groundingdino.util.utils",
        "description": "groundingdino.util.utils",
        "isExtraImport": true,
        "detail": "groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "get_phrases_from_posmap",
        "importPath": "groundingdino.util.utils",
        "description": "groundingdino.util.utils",
        "isExtraImport": true,
        "detail": "groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "COCOVisualizer",
        "importPath": "groundingdino.util.visualizer",
        "description": "groundingdino.util.visualizer",
        "isExtraImport": true,
        "detail": "groundingdino.util.visualizer",
        "documentation": {}
    },
    {
        "label": "create_positive_map_from_span",
        "importPath": "groundingdino.util.vl_utils",
        "description": "groundingdino.util.vl_utils",
        "isExtraImport": true,
        "detail": "groundingdino.util.vl_utils",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "once_differentiable",
        "importPath": "torch.autograd.function",
        "description": "torch.autograd.function",
        "isExtraImport": true,
        "detail": "torch.autograd.function",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "groundingdino.datasets.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "groundingdino.datasets.transforms",
        "description": "groundingdino.datasets.transforms",
        "detail": "groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "supervision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "supervision",
        "description": "supervision",
        "detail": "supervision",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "groundingdino.models",
        "description": "groundingdino.models",
        "isExtraImport": true,
        "detail": "groundingdino.models",
        "documentation": {}
    },
    {
        "label": "SLConfig",
        "importPath": "groundingdino.util.slconfig",
        "description": "groundingdino.util.slconfig",
        "isExtraImport": true,
        "detail": "groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "SLConfig",
        "importPath": "groundingdino.util.slconfig",
        "description": "groundingdino.util.slconfig",
        "isExtraImport": true,
        "detail": "groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "box_convert",
        "importPath": "torchvision.ops",
        "description": "torchvision.ops",
        "isExtraImport": true,
        "detail": "torchvision.ops",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "colored",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "colorsys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "colorsys",
        "description": "colorsys",
        "detail": "colorsys",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Action",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "import_module",
        "importPath": "importlib",
        "description": "importlib",
        "isExtraImport": true,
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "addict",
        "description": "addict",
        "isExtraImport": true,
        "detail": "addict",
        "documentation": {}
    },
    {
        "label": "FormatCode",
        "importPath": "yapf.yapflib.yapf_api",
        "description": "yapf.yapflib.yapf_api",
        "isExtraImport": true,
        "detail": "yapf.yapflib.yapf_api",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "PatchCollection",
        "importPath": "matplotlib.collections",
        "description": "matplotlib.collections",
        "isExtraImport": true,
        "detail": "matplotlib.collections",
        "documentation": {}
    },
    {
        "label": "Polygon",
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "isExtraImport": true,
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "mask",
        "importPath": "pycocotools",
        "description": "pycocotools",
        "isExtraImport": true,
        "detail": "pycocotools",
        "documentation": {}
    },
    {
        "label": "PIPELINES",
        "importPath": "modelscope.pipelines.builder",
        "description": "modelscope.pipelines.builder",
        "isExtraImport": true,
        "detail": "modelscope.pipelines.builder",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "importPath": "modelscope.models.builder",
        "description": "modelscope.models.builder",
        "isExtraImport": true,
        "detail": "modelscope.models.builder",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "modelscope.pipelines.base",
        "description": "modelscope.pipelines.base",
        "isExtraImport": true,
        "detail": "modelscope.pipelines.base",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope.models.base",
        "description": "modelscope.models.base",
        "isExtraImport": true,
        "detail": "modelscope.models.base",
        "documentation": {}
    },
    {
        "label": "TorchModel",
        "importPath": "modelscope.models.base",
        "description": "modelscope.models.base",
        "isExtraImport": true,
        "detail": "modelscope.models.base",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "modelscope.utils.logger",
        "description": "modelscope.utils.logger",
        "isExtraImport": true,
        "detail": "modelscope.utils.logger",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "groundingdino.util.inference",
        "description": "groundingdino.util.inference",
        "isExtraImport": true,
        "detail": "groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "load_image",
        "importPath": "groundingdino.util.inference",
        "description": "groundingdino.util.inference",
        "isExtraImport": true,
        "detail": "groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "predict",
        "importPath": "groundingdino.util.inference",
        "description": "groundingdino.util.inference",
        "isExtraImport": true,
        "detail": "groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "annotate",
        "importPath": "groundingdino.util.inference",
        "description": "groundingdino.util.inference",
        "isExtraImport": true,
        "detail": "groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "AutoModelForZeroShotObjectDetection",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"screen_loc\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"screen_loc\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"screen_loc\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"screen_loc\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"screen_loc\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "batch_size = 1\nmodelname = 'groundingdino'\nbackbone = 'swin_T_224_1k'\nposition_embedding = 'sine'\npe_temperatureH = 20\npe_temperatureW = 20\nreturn_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "modelname",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "modelname = 'groundingdino'\nbackbone = 'swin_T_224_1k'\nposition_embedding = 'sine'\npe_temperatureH = 20\npe_temperatureW = 20\nreturn_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "backbone",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "backbone = 'swin_T_224_1k'\nposition_embedding = 'sine'\npe_temperatureH = 20\npe_temperatureW = 20\nreturn_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "position_embedding",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "position_embedding = 'sine'\npe_temperatureH = 20\npe_temperatureW = 20\nreturn_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "pe_temperatureH",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "pe_temperatureH = 20\npe_temperatureW = 20\nreturn_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "pe_temperatureW",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "pe_temperatureW = 20\nreturn_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "return_interm_indices",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "return_interm_indices = [1, 2, 3]\nbackbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "backbone_freeze_keywords",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "backbone_freeze_keywords = None\nenc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "enc_layers",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "enc_layers = 6\ndec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dec_layers",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dec_layers = 6\npre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "pre_norm",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "pre_norm = False\ndim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dim_feedforward",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dim_feedforward = 2048\nhidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "hidden_dim",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "hidden_dim = 256\ndropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dropout",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dropout = 0.0\nnheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "nheads",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "nheads = 8\nnum_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "num_queries",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "num_queries = 900\nquery_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "query_dim",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "query_dim = 4\nnum_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "num_patterns",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "num_patterns = 0\nnum_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "num_feature_levels",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "num_feature_levels = 4\nenc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "enc_n_points",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "enc_n_points = 4\ndec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dec_n_points",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dec_n_points = 4\ntwo_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "two_stage_type",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "two_stage_type = 'standard'\ntwo_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "two_stage_bbox_embed_share",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "two_stage_bbox_embed_share = False\ntwo_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "two_stage_class_embed_share",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "two_stage_class_embed_share = False\ntransformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "transformer_activation",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "transformer_activation = 'relu'\ndec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dec_pred_bbox_embed_share",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dec_pred_bbox_embed_share = True\ndn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dn_box_noise_scale",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dn_box_noise_scale = 1.0\ndn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dn_label_noise_ratio",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dn_label_noise_ratio = 0.5\ndn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dn_label_coef",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dn_label_coef = 1.0\ndn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dn_bbox_coef",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dn_bbox_coef = 1.0\nembed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "embed_init_tgt",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "embed_init_tgt = True\ndn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "dn_labelbook_size",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "dn_labelbook_size = 2000\nmax_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "max_text_len",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "max_text_len = 256\ntext_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "text_encoder_type",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "text_encoder_type = 'bert-base-uncased'\n# text_encoder_type = \"examples/mobile_agent/bert-base-uncased\"\nuse_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "use_text_enhancer",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "use_text_enhancer = True\nuse_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "use_fusion_layer",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "use_fusion_layer = True\nuse_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "use_checkpoint",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "use_checkpoint = True\nuse_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "use_transformer_ckpt",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "use_transformer_ckpt = True\nuse_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "use_text_cross_attention",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "use_text_cross_attention = True\ntext_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "text_dropout",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "text_dropout = 0.0\nfusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "fusion_dropout",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "fusion_dropout = 0.0\nfusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "fusion_droppath",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "fusion_droppath = 0.1\nsub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "sub_sentence_present",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "peekOfCode": "sub_sentence_present = True",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.config.GroundingDINO_SwinT_OGC",
        "documentation": {}
    },
    {
        "label": "ResizeDebug",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class ResizeDebug(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        return resize(img, target, self.size)\nclass RandomCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        region = T.RandomCrop.get_params(img, self.size)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomCrop",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        region = T.RandomCrop.get_params(img, self.size)\n        return crop(img, target, region)\nclass RandomSizeCrop(object):\n    def __init__(self,\n                 min_size: int,\n                 max_size: int,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSizeCrop",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomSizeCrop(object):\n    def __init__(self,\n                 min_size: int,\n                 max_size: int,\n                 respect_boxes: bool = False):\n        # respect_boxes:    True to keep all boxes\n        #                   False to tolerence box filter\n        self.min_size = min_size\n        self.max_size = max_size\n        self.respect_boxes = respect_boxes",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class CenterCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        image_width, image_height = img.size\n        crop_height, crop_width = self.size\n        crop_top = int(round((image_height - crop_height) / 2.0))\n        crop_left = int(round((image_width - crop_width) / 2.0))\n        return crop(img, target,\n                    (crop_top, crop_left, crop_height, crop_width))",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomHorizontalFlip",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img, target):\n        if random.random() < self.p:\n            return hflip(img, target)\n        return img, target\nclass RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomResize",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))\n        self.sizes = sizes\n        self.max_size = max_size\n    def __call__(self, img, target=None):\n        size = random.choice(self.sizes)\n        return resize(img, target, size, self.max_size)\nclass RandomPad(object):\n    def __init__(self, max_pad):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomPad",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomPad(object):\n    def __init__(self, max_pad):\n        self.max_pad = max_pad\n    def __call__(self, img, target):\n        pad_x = random.randint(0, self.max_pad)\n        pad_y = random.randint(0, self.max_pad)\n        return pad(img, target, (pad_x, pad_y))\nclass RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSelect",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,\n    with probability p for transforms1 and (1 - p) for transforms2\n    \"\"\"\n    def __init__(self, transforms1, transforms2, p=0.5):\n        self.transforms1 = transforms1\n        self.transforms2 = transforms2\n        self.p = p\n    def __call__(self, img, target):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class ToTensor(object):\n    def __call__(self, img, target):\n        return F.to_tensor(img), target\nclass RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomErasing",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        if target is None:\n            return image, None\n        target = target.copy()\n        h, w = image.shape[-2:]",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "crop",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "def crop(image, target, region):\n    cropped_image = F.crop(image, *region)\n    target = target.copy()\n    i, j, h, w = region\n    # should we do something wrt the original size?\n    target['size'] = torch.tensor([h, w])\n    fields = ['labels', 'area', 'iscrowd', 'positive_map']\n    if 'boxes' in target:\n        boxes = target['boxes']\n        max_size = torch.as_tensor([w, h], dtype=torch.float32)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "hflip",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "def hflip(image, target):\n    flipped_image = F.hflip(image)\n    w, h = image.size\n    target = target.copy()\n    if 'boxes' in target:\n        boxes = target['boxes']\n        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor(\n            [-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n        target['boxes'] = boxes\n    if 'masks' in target:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "def resize(image, target, size, max_size=None):\n    # size can be min_size (scalar) or (w, h) tuple\n    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n        w, h = image_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(\n                    round(max_size * min_original_size / max_original_size))",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "peekOfCode": "def pad(image, target, padding):\n    # assumes that we only pad on the bottom right corners\n    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n    if target is None:\n        return padded_image, None\n    target = target.copy()\n    # should we do something wrt the original size?\n    target['size'] = torch.tensor(padded_image.size[::-1])\n    if 'masks' in target:\n        target['masks'] = torch.nn.functional.pad(",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "FrozenBatchNorm2d",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "peekOfCode": "class FrozenBatchNorm2d(torch.nn.Module):\n    \"\"\"\n    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n    without which any other models than torchvision.models.resnet[18,34,50,101]\n    produce nans.\n    \"\"\"\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer('weight', torch.ones(n))",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "documentation": {}
    },
    {
        "label": "BackboneBase",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "peekOfCode": "class BackboneBase(nn.Module):\n    def __init__(\n        self,\n        backbone: nn.Module,\n        train_backbone: bool,\n        num_channels: int,\n        return_interm_indices: list,\n    ):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "peekOfCode": "class Backbone(BackboneBase):\n    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n    def __init__(\n        self,\n        name: str,\n        train_backbone: bool,\n        dilation: bool,\n        return_interm_indices: list,\n        batch_norm=FrozenBatchNorm2d,\n    ):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "peekOfCode": "class Joiner(nn.Sequential):\n    def __init__(self, backbone, position_embedding):\n        super().__init__(backbone, position_embedding)\n    def forward(self, tensor_list: NestedTensor):\n        xs = self[0](tensor_list)\n        out: List[NestedTensor] = []\n        pos = []\n        for name, x in xs.items():\n            out.append(x)\n            # position encoding",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "peekOfCode": "def build_backbone(args):\n    \"\"\"\n    Useful args:\n        - backbone: backbone name\n        - lr_backbone:\n        - dilation\n        - return_interm_indices: available: [0,1,2,3], [1,2,3], [3]\n        - backbone_freeze_keywords:\n        - use_checkpoint: for swin only for now\n    \"\"\"",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.backbone",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "peekOfCode": "class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self,\n                 num_pos_feats=64,\n                 temperature=10000,\n                 normalize=False,\n                 scale=None):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSineHW",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "peekOfCode": "class PositionEmbeddingSineHW(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self,\n                 num_pos_feats=64,\n                 temperatureH=10000,\n                 temperatureW=10000,\n                 normalize=False,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingLearned",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "peekOfCode": "class PositionEmbeddingLearned(nn.Module):\n    \"\"\"\n    Absolute pos embedding, learned.\n    \"\"\"\n    def __init__(self, num_pos_feats=256):\n        super().__init__()\n        self.row_embed = nn.Embedding(50, num_pos_feats)\n        self.col_embed = nn.Embedding(50, num_pos_feats)\n        self.reset_parameters()\n    def reset_parameters(self):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "peekOfCode": "def build_position_encoding(args):\n    N_steps = args.hidden_dim // 2\n    if args.position_embedding in ('v2', 'sine'):\n        # TODO find a better way of exposing other arguments\n        position_embedding = PositionEmbeddingSineHW(\n            N_steps,\n            temperatureH=args.pe_temperatureH,\n            temperatureW=args.pe_temperatureW,\n            normalize=True,\n        )",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    \"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (int): Local window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\n    Args:\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self,\n                 patch_size=4,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "SwinTransformer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "class SwinTransformer(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        pretrain_img_size (int): Input image size for training the pretrained model,\n            used in absolute postion embedding. Default 224.\n        patch_size (int | tuple(int)): Patch size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "build_swin_transformer",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "peekOfCode": "def build_swin_transformer(modelname, pretrain_img_size, **kw):\n    assert modelname in [\n        'swin_T_224_1k',\n        'swin_B_224_22k',\n        'swin_B_384_22k',\n        'swin_L_224_22k',\n        'swin_L_384_22k',\n    ]\n    model_para_dict = {\n        'swin_T_224_1k':",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.backbone.swin_transformer",
        "documentation": {}
    },
    {
        "label": "BertModelWarper",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "peekOfCode": "class BertModelWarper(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        # self.bert = bert_modelc\n        self.config = bert_model.config\n        self.embeddings = bert_model.embeddings\n        self.encoder = bert_model.encoder\n        self.pooler = bert_model.pooler\n        self.get_extended_attention_mask = bert_model.get_extended_attention_mask\n        self.invert_attention_mask = bert_model.invert_attention_mask",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "documentation": {}
    },
    {
        "label": "TextEncoderShell",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "peekOfCode": "class TextEncoderShell(nn.Module):\n    def __init__(self, text_encoder):\n        super().__init__()\n        self.text_encoder = text_encoder\n        self.config = self.text_encoder.config\n    def forward(self, **kw):\n        # feed into text encoder\n        return self.text_encoder(**kw)\ndef generate_masks_with_special_tokens(tokenized, special_tokens_list,\n                                       tokenizer):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "documentation": {}
    },
    {
        "label": "generate_masks_with_special_tokens",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "peekOfCode": "def generate_masks_with_special_tokens(tokenized, special_tokens_list,\n                                       tokenizer):\n    \"\"\"Generate attention mask between each pair of special tokens\n    Args:\n        input_ids (torch.Tensor): input ids. Shape: [bs, num_token]\n        special_tokens_mask (list): special tokens mask.\n    Returns:\n        torch.Tensor: attention mask between each special tokens.\n    \"\"\"\n    input_ids = tokenized['input_ids']",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "documentation": {}
    },
    {
        "label": "generate_masks_with_special_tokens_and_transfer_map",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "peekOfCode": "def generate_masks_with_special_tokens_and_transfer_map(\n        tokenized, special_tokens_list, tokenizer):\n    \"\"\"Generate attention mask between each pair of special tokens\n    Args:\n        input_ids (torch.Tensor): input ids. Shape: [bs, num_token]\n        special_tokens_mask (list): special tokens mask.\n    Returns:\n        torch.Tensor: attention mask between each special tokens.\n    \"\"\"\n    input_ids = tokenized['input_ids']",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper",
        "documentation": {}
    },
    {
        "label": "FeatureResizer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "peekOfCode": "class FeatureResizer(nn.Module):\n    \"\"\"\n    This class takes as input a set of embeddings of dimension C1 and outputs a set of\n    embedding of dimension C2, after a linear transformation, dropout and normalization (LN).\n    \"\"\"\n    def __init__(self, input_feat_size, output_feat_size, dropout, do_ln=True):\n        super().__init__()\n        self.do_ln = do_ln\n        # Object feature encoding\n        self.fc = nn.Linear(input_feat_size, output_feat_size, bias=True)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "documentation": {}
    },
    {
        "label": "BiMultiHeadAttention",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "peekOfCode": "class BiMultiHeadAttention(nn.Module):\n    def __init__(self,\n                 v_dim,\n                 l_dim,\n                 embed_dim,\n                 num_heads,\n                 dropout=0.1,\n                 cfg=None):\n        super(BiMultiHeadAttention, self).__init__()\n        self.embed_dim = embed_dim",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "documentation": {}
    },
    {
        "label": "BiAttentionBlock",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "peekOfCode": "class BiAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        v_dim,\n        l_dim,\n        embed_dim,\n        num_heads,\n        dropout=0.1,\n        drop_path=0.0,\n        init_values=1e-4,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "documentation": {}
    },
    {
        "label": "l1norm",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "peekOfCode": "def l1norm(X, dim, eps=1e-8):\n    \"\"\"L1-normalize columns of X\"\"\"\n    norm = torch.abs(X).sum(dim=dim, keepdim=True) + eps\n    X = torch.div(X, norm)\n    return X\ndef l2norm(X, dim, eps=1e-8):\n    \"\"\"L2-normalize columns of X\"\"\"\n    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps\n    X = torch.div(X, norm)\n    return X",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "documentation": {}
    },
    {
        "label": "l2norm",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "peekOfCode": "def l2norm(X, dim, eps=1e-8):\n    \"\"\"L2-normalize columns of X\"\"\"\n    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps\n    X = torch.div(X, norm)\n    return X\ndef func_attention(query,\n                   context,\n                   smooth=1,\n                   raw_feature_norm='softmax',\n                   eps=1e-8):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "documentation": {}
    },
    {
        "label": "func_attention",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "peekOfCode": "def func_attention(query,\n                   context,\n                   smooth=1,\n                   raw_feature_norm='softmax',\n                   eps=1e-8):\n    \"\"\"\n    query: (n_context, queryL, d)\n    context: (n_context, sourceL, d)\n    \"\"\"\n    batch_size_q, queryL = query.size(0), query.size(1)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.fuse_modules",
        "documentation": {}
    },
    {
        "label": "GroundingDINO",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.groundingdino",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.groundingdino",
        "peekOfCode": "class GroundingDINO(nn.Module):\n    \"\"\"This is the Cross-Attention Detector module that performs object detection\"\"\"\n    def __init__(\n        self,\n        backbone,\n        transformer,\n        num_queries,\n        aux_loss=False,\n        iter_update=False,\n        query_dim=2,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.groundingdino",
        "documentation": {}
    },
    {
        "label": "build_groundingdino",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.groundingdino",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.groundingdino",
        "peekOfCode": "def build_groundingdino(args):\n    backbone = build_backbone(args)\n    transformer = build_transformer(args)\n    dn_labelbook_size = args.dn_labelbook_size\n    dec_pred_bbox_embed_share = args.dec_pred_bbox_embed_share\n    sub_sentence_present = args.sub_sentence_present\n    model = GroundingDINO(\n        backbone,\n        transformer,\n        num_queries=args.num_queries,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.groundingdino",
        "documentation": {}
    },
    {
        "label": "MultiScaleDeformableAttnFunction",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "peekOfCode": "class MultiScaleDeformableAttnFunction(Function):\n    @staticmethod\n    def forward(\n        ctx,\n        value,\n        value_spatial_shapes,\n        value_level_start_index,\n        sampling_locations,\n        attention_weights,\n        im2col_step,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "MultiScaleDeformableAttention",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "peekOfCode": "class MultiScaleDeformableAttention(nn.Module):\n    \"\"\"Multi-Scale Deformable Attention Module used in Deformable-DETR\n    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.\n    <https://arxiv.org/pdf/2010.04159.pdf>`_.\n    Args:\n        embed_dim (int): The embedding dimension of Attention. Default: 256.\n        num_heads (int): The number of attention heads. Default: 8.\n        num_levels (int): The number of feature map used in Attention. Default: 4.\n        num_points (int): The number of sampling points for each query\n            in each head. Default: 4.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "multi_scale_deformable_attn_pytorch",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "peekOfCode": "def multi_scale_deformable_attn_pytorch(\n    value: torch.Tensor,\n    value_spatial_shapes: torch.Tensor,\n    sampling_locations: torch.Tensor,\n    attention_weights: torch.Tensor,\n) -> torch.Tensor:\n    bs, _, num_heads, embed_dims = value.shape\n    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes],\n                             dim=1)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "create_dummy_class",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "peekOfCode": "def create_dummy_class(klass, dependency, message=''):\n    \"\"\"\n    When a dependency of a class is not available, create a dummy class which throws ImportError\n    when used.\n    Args:\n        klass (str): name of the class.\n        dependency (str): name of the dependency.\n        message: extra message to print\n    Returns:\n        class: a class object",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "create_dummy_func",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "peekOfCode": "def create_dummy_func(func, dependency, message=''):\n    \"\"\"\n    When a dependency of a function is not available, create a dummy function which throws\n    ImportError when used.\n    Args:\n        func (str): name of the function.\n        dependency (str or list[str]): name(s) of the dependency.\n        message: extra message to print\n    Returns:\n        function: a function object",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(\n        self,\n        d_model=256,\n        nhead=8,\n        num_queries=300,\n        num_encoder_layers=6,\n        num_unicoder_layers=0,\n        num_decoder_layers=6,\n        dim_feedforward=2048,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerEncoder",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "peekOfCode": "class TransformerEncoder(nn.Module):\n    def __init__(\n        self,\n        encoder_layer,\n        num_layers,\n        d_model=256,\n        num_queries=300,\n        enc_layer_share=False,\n        text_enhance_layer=None,\n        feature_fusion_layer=None,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerDecoder",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "peekOfCode": "class TransformerDecoder(nn.Module):\n    def __init__(\n        self,\n        decoder_layer,\n        num_layers,\n        norm=None,\n        return_intermediate=False,\n        d_model=256,\n        query_dim=4,\n        num_feature_levels=1,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerEncoderLayer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "peekOfCode": "class DeformableTransformerEncoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model=256,\n        d_ffn=1024,\n        dropout=0.1,\n        activation='relu',\n        n_levels=4,\n        n_heads=8,\n        n_points=4,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerDecoderLayer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "peekOfCode": "class DeformableTransformerDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model=256,\n        d_ffn=1024,\n        dropout=0.1,\n        activation='relu',\n        n_levels=4,\n        n_heads=8,\n        n_points=4,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "peekOfCode": "def build_transformer(args):\n    return Transformer(\n        d_model=args.hidden_dim,\n        dropout=args.dropout,\n        nhead=args.nheads,\n        num_queries=args.num_queries,\n        dim_feedforward=args.dim_feedforward,\n        num_encoder_layers=args.enc_layers,\n        num_decoder_layers=args.dec_layers,\n        normalize_before=args.pre_norm,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer",
        "documentation": {}
    },
    {
        "label": "TextTransformer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer_vanilla",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer_vanilla",
        "peekOfCode": "class TextTransformer(nn.Module):\n    def __init__(self,\n                 num_layers,\n                 d_model=256,\n                 nheads=8,\n                 dim_feedforward=2048,\n                 dropout=0.1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.d_model = d_model",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer_vanilla",
        "documentation": {}
    },
    {
        "label": "TransformerEncoderLayer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer_vanilla",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer_vanilla",
        "peekOfCode": "class TransformerEncoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation='relu',\n        normalize_before=False,\n    ):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.transformer_vanilla",
        "documentation": {}
    },
    {
        "label": "RandomBoxPerturber",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "class RandomBoxPerturber:\n    def __init__(self,\n                 x_noise_scale=0.2,\n                 y_noise_scale=0.2,\n                 w_noise_scale=0.2,\n                 h_noise_scale=0.2) -> None:\n        self.noise_scale = torch.Tensor(\n            [x_noise_scale, y_noise_scale, w_noise_scale, h_noise_scale])\n    def __call__(self, refanchors: Tensor) -> Tensor:\n        nq, bs, query_dim = refanchors.shape",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\"Very simple multi-layer perceptron (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(\n            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "ContrastiveEmbed",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "class ContrastiveEmbed(nn.Module):\n    def __init__(self, max_text_len=256):\n        \"\"\"\n        Args:\n            max_text_len: max length of text.\n        \"\"\"\n        super().__init__()\n        self.max_text_len = max_text_len\n    def forward(self, x, text_dict):\n        \"\"\"_summary_",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "get_sine_pos_embed",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "def get_sine_pos_embed(\n    pos_tensor: torch.Tensor,\n    num_pos_feats: int = 128,\n    temperature: int = 10000,\n    exchange_xy: bool = True,\n):\n    \"\"\"generate sine position embedding from a position tensor\n    Args:\n        pos_tensor (torch.Tensor): shape: [..., n].\n        num_pos_feats (int): projected shape for each float in the tensor.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "gen_encoder_output_proposals",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "def gen_encoder_output_proposals(memory: Tensor,\n                                 memory_padding_mask: Tensor,\n                                 spatial_shapes: Tensor,\n                                 learnedwh=None):\n    \"\"\"\n    Input:\n        - memory: bs, \\sum{hw}, d_model\n        - memory_padding_mask: bs, \\sum{hw}\n        - spatial_shapes: nlevel, 2\n        - learnedwh: 2",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "sigmoid_focal_loss",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "def sigmoid_focal_loss(inputs,\n                       targets,\n                       num_boxes,\n                       alpha: float = 0.25,\n                       gamma: float = 2,\n                       no_reduction=False):\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Args:\n        inputs: A float tensor of arbitrary shape.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "gen_sineembed_for_position",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "peekOfCode": "def gen_sineembed_for_position(pos_tensor):\n    # n_query, bs, _ = pos_tensor.size()\n    # sineembed_tensor = torch.zeros(n_query, bs, 256)\n    scale = 2 * math.pi\n    dim_t = torch.arange(128, dtype=torch.float32, device=pos_tensor.device)\n    dim_t = 10000**(2 * (torch.div(dim_t, 2, rounding_mode='floor')) / 128)\n    x_embed = pos_tensor[:, :, 0] * scale\n    y_embed = pos_tensor[:, :, 1] * scale\n    pos_x = x_embed[:, :, None] / dim_t\n    pos_y = y_embed[:, :, None] / dim_t",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.GroundingDINO.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.registry",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.registry",
        "peekOfCode": "class Registry(object):\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n    def __repr__(self):\n        format_str = self.__class__.__name__ + '(name={}, items={})'.format(\n            self._name, list(self._module_dict.keys()))\n        return format_str\n    def __len__(self):\n        return len(self._module_dict)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.registry",
        "documentation": {}
    },
    {
        "label": "MODULE_BUILD_FUNCS",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.models.registry",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.models.registry",
        "peekOfCode": "MODULE_BUILD_FUNCS = Registry('model build functions')",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.models.registry",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=-1)\ndef box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)\n# modified from torchvision to also return the union\ndef box_iou(boxes1, boxes2):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)\n# modified from torchvision to also return the union\ndef box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    # import ipdb; ipdb.set_trace()\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    # import ipdb; ipdb.set_trace()\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n    union = area1[:, None] + area2 - inter\n    iou = inter / (union + 1e-6)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def generalized_box_iou(boxes1, boxes2):\n    \"\"\"\n    Generalized IoU from https://giou.stanford.edu/\n    The boxes should be in [x0, y0, x1, y1] format\n    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n    and M = len(boxes2)\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check\n    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou_pairwise",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def box_iou_pairwise(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, :2], boxes2[:, :2])  # [N,2]\n    rb = torch.min(boxes1[:, 2:], boxes2[:, 2:])  # [N,2]\n    wh = (rb - lt).clamp(min=0)  # [N,2]\n    inter = wh[:, 0] * wh[:, 1]  # [N]\n    union = area1 + area2 - inter\n    iou = inter / union\n    return iou, union",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou_pairwise",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def generalized_box_iou_pairwise(boxes1, boxes2):\n    \"\"\"\n    Generalized IoU from https://giou.stanford.edu/\n    Input:\n        - boxes1, boxes2: N,4\n    Output:\n        - giou: N, 4\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "peekOfCode": "def masks_to_boxes(masks):\n    \"\"\"Compute the bounding boxes around the provided masks\n    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n    Returns a [N, 4] tensors, with the boxes in xyxy format\n    \"\"\"\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device)\n    h, w = masks.shape[-2:]\n    y = torch.arange(0, h, dtype=torch.float)\n    x = torch.arange(0, w, dtype=torch.float)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.box_ops",
        "documentation": {}
    },
    {
        "label": "get_tokenlizer",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.get_tokenlizer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.get_tokenlizer",
        "peekOfCode": "def get_tokenlizer(text_encoder_type):\n    if not isinstance(text_encoder_type, str):\n        # print(\"text_encoder_type is not a str\")\n        if hasattr(text_encoder_type, 'text_encoder_type'):\n            text_encoder_type = text_encoder_type.text_encoder_type\n        elif text_encoder_type.get('text_encoder_type', False):\n            text_encoder_type = text_encoder_type.get('text_encoder_type')\n        else:\n            raise ValueError('Unknown type of text_encoder_type: {}'.format(\n                type(text_encoder_type)))",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.get_tokenlizer",
        "documentation": {}
    },
    {
        "label": "get_pretrained_language_model",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.get_tokenlizer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.get_tokenlizer",
        "peekOfCode": "def get_pretrained_language_model(text_encoder_type):\n    return BertModel.from_pretrained(text_encoder_type)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.get_tokenlizer",
        "documentation": {}
    },
    {
        "label": "preprocess_caption",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "peekOfCode": "def preprocess_caption(caption: str) -> str:\n    result = caption.lower().strip()\n    if result.endswith('.'):\n        return result\n    return result + '.'\ndef load_model(model_config_path: str,\n               model_checkpoint_path: str,\n               device: str = 'cuda'):\n    args = SLConfig.fromfile(model_config_path)\n    args.device = device",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "peekOfCode": "def load_model(model_config_path: str,\n               model_checkpoint_path: str,\n               device: str = 'cuda'):\n    args = SLConfig.fromfile(model_config_path)\n    args.device = device\n    # args.model_dir = model_dir\n    model = build_model(args)\n    checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n    model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n    model.eval()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "peekOfCode": "def load_image(image_path: str) -> Tuple[np.array, torch.Tensor]:\n    transform = T.Compose([\n        T.RandomResize([800], max_size=1333),\n        T.ToTensor(),\n        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    image_source = Image.open(image_path).convert('RGB')\n    image = np.asarray(image_source)\n    image_transformed, _ = transform(image_source, None)\n    return image, image_transformed",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "peekOfCode": "def predict(\n        model,\n        image: torch.Tensor,\n        caption: str,\n        box_threshold: float,\n        text_threshold: float,\n        device: str = 'cuda') -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n    caption = preprocess_caption(caption=caption)\n    model = model.to(device)\n    image = image.to(device)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "annotate",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "peekOfCode": "def annotate(image_source: np.ndarray, boxes: torch.Tensor,\n             logits: torch.Tensor, phrases: List[str]) -> np.ndarray:\n    h, w, _ = image_source.shape\n    boxes = boxes * torch.Tensor([w, h, w, h])\n    xyxy = box_convert(boxes=boxes, in_fmt='cxcywh', out_fmt='xyxy').numpy()\n    detections = sv.Detections(xyxy=xyxy)\n    labels = [\n        f'{phrase} {logit:.2f}' for phrase, logit in zip(phrases, logits)\n    ]\n    box_annotator = sv.BoxAnnotator()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.inference",
        "documentation": {}
    },
    {
        "label": "_ColorfulFormatter",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.logger",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.logger",
        "peekOfCode": "class _ColorfulFormatter(logging.Formatter):\n    def __init__(self, *args, **kwargs):\n        self._root_name = kwargs.pop('root_name') + '.'\n        self._abbrev_name = kwargs.pop('abbrev_name', '')\n        if len(self._abbrev_name):\n            self._abbrev_name = self._abbrev_name + '.'\n        super(_ColorfulFormatter, self).__init__(*args, **kwargs)\n    def formatMessage(self, record):\n        record.name = record.name.replace(self._root_name, self._abbrev_name)\n        log = super(_ColorfulFormatter, self).formatMessage(record)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.logger",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.logger",
        "peekOfCode": "def setup_logger(output=None,\n                 distributed_rank=0,\n                 *,\n                 color=True,\n                 name='imagenet',\n                 abbrev_name=None):\n    \"\"\"\n    Initialize the detectron2 logger and set its verbosity level to \"INFO\".\n    Args:\n        output (str): a file name or a directory to save log. If None, will not save log file.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.logger",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = '{median:.4f} ({global_avg:.4f})'\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter='\\t'):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "class NestedTensor(object):\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n        if mask == 'auto':\n            self.mask = torch.zeros_like(tensors).to(tensors.device)\n            if self.mask.dim() == 3:\n                self.mask = self.mask.sum(0).to(bool)\n            elif self.mask.dim() == 4:\n                self.mask = self.mask.sum(1).to(bool)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "color_sys",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "class color_sys:\n    def __init__(self, num_colors) -> None:\n        self.num_colors = num_colors\n        colors = []\n        for i in np.arange(0.0, 360.0, 360.0 / num_colors):\n            hue = i / 360.0\n            lightness = (50 + np.random.rand() * 10) / 100.0\n            saturation = (90 + np.random.rand() * 10) / 100.0\n            colors.append(\n                tuple([",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather_cpu",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def all_gather_cpu(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    world_size = get_world_size()\n    if world_size == 1:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    if os.getenv('CPU_REDUCE') == '1':\n        return all_gather_cpu(data)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "reduce_dict",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(\n            command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = 'clean'\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "collate_fn",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def collate_fn(batch):\n    # import ipdb; ipdb.set_trace()\n    batch = list(zip(*batch))\n    batch[0] = nested_tensor_from_tensor_list(batch[0])\n    return tuple(batch)\ndef _max_by_axis(the_list):\n    # type: (List[List[int]]) -> List[int]\n    maxes = the_list[0]\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    # TODO make this more general\n    if tensor_list[0].ndim == 3:\n        if torchvision._is_tracing():\n            # nested_tensor_from_tensor_list() does not export well to ONNX\n            # call _onnx_nested_tensor_from_tensor_list() instead\n            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n        # TODO make it support different-sized images\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'WORLD_SIZE' in os.environ and os.environ[\n            'WORLD_SIZE'] != '':  # 'RANK' in os.environ and\n        args.rank = int(os.environ['RANK'])\n        args.world_size = int(os.environ['WORLD_SIZE'])",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "save_on_master",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'WORLD_SIZE' in os.environ and os.environ[\n            'WORLD_SIZE'] != '':  # 'RANK' in os.environ and\n        args.rank = int(os.environ['RANK'])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = args.local_rank = int(os.environ['LOCAL_RANK'])\n        # launch by torch.distributed.launch",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "init_distributed_mode",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def init_distributed_mode(args):\n    if 'WORLD_SIZE' in os.environ and os.environ[\n            'WORLD_SIZE'] != '':  # 'RANK' in os.environ and\n        args.rank = int(os.environ['RANK'])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = args.local_rank = int(os.environ['LOCAL_RANK'])\n        # launch by torch.distributed.launch\n        # Single node\n        #   python -m torch.distributed.launch --nproc_per_node=8 main.py --world-size 1 --rank 0 ...\n        # Multi nodes",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def accuracy(output, target, topk=(1, )):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    if target.numel() == 0:\n        return [torch.zeros([], device=output.device)]\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy_onehot",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def accuracy_onehot(pred, gt):\n    \"\"\"_summary_\n    Args:\n        pred (_type_): n, c\n        gt (_type_): n, c\n    \"\"\"\n    tp = ((pred - gt).abs().sum(-1) < 1e-4).float().sum()\n    acc = tp / gt.shape[0] * 100\n    return acc\ndef interpolate(input,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def interpolate(input,\n                size=None,\n                scale_factor=None,\n                mode='nearest',\n                align_corners=None):\n    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n    \"\"\"\n    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n    This will eventually be supported natively by PyTorch, and this\n    class can go away.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def inverse_sigmoid(x, eps=1e-3):\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)\n    x2 = (1 - x).clamp(min=eps)\n    return torch.log(x1 / x2)\ndef clean_state_dict(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        if k[:7] == 'module.':\n            k = k[7:]  # remove `module.`",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "clean_state_dict",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "def clean_state_dict(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        if k[:7] == 'module.':\n            k = k[7:]  # remove `module.`\n        new_state_dict[k] = v\n    return new_state_dict",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "__torchvision_need_compat_flag",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "peekOfCode": "__torchvision_need_compat_flag = float(\n    torchvision.__version__.split('.')[1]) < 7\nif __torchvision_need_compat_flag:\n    from torchvision.ops import _new_empty_tensor\n    from torchvision.ops.misc import _output_size\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.misc",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "class ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)\n    def __getattr__(self, name):\n        try:\n            value = super(ConfigDict, self).__getattr__(name)\n        except KeyError:\n            ex = AttributeError(f\"'{self.__class__.__name__}' object has no \"\n                                f\"attribute '{name}'\")\n        except Exception as e:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "SLConfig",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "class SLConfig(object):\n    \"\"\"\n    config files.\n    only support .py file as config now.\n    ref: mmcv.utils.config\n    Example:\n        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n        >>> cfg.a\n        1\n        >>> cfg.b",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "DictAction",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "class DictAction(Action):\n    \"\"\"\n    argparse action to split an argument into KEY=VALUE form\n    on the first = and append to a dictionary. List options should\n    be passed as comma separated values, i.e KEY=V1,V2,V3\n    \"\"\"\n    @staticmethod\n    def _parse_int_float_bool(val):\n        try:\n            return int(val)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "check_file_exist",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "def check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\nclass ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)\n    def __getattr__(self, name):\n        try:\n            value = super(ConfigDict, self).__getattr__(name)\n        except KeyError:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "BASE_KEY",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "BASE_KEY = '_base_'\nDELETE_KEY = '_delete_'\nRESERVED_KEYS = [\n    'filename', 'text', 'pretty_text', 'get', 'dump', 'merge_from_dict'\n]\ndef check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\nclass ConfigDict(Dict):\n    def __missing__(self, name):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "DELETE_KEY",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "DELETE_KEY = '_delete_'\nRESERVED_KEYS = [\n    'filename', 'text', 'pretty_text', 'get', 'dump', 'merge_from_dict'\n]\ndef check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\nclass ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "RESERVED_KEYS",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "peekOfCode": "RESERVED_KEYS = [\n    'filename', 'text', 'pretty_text', 'get', 'dump', 'merge_from_dict'\n]\ndef check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\nclass ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)\n    def __getattr__(self, name):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slconfig",
        "documentation": {}
    },
    {
        "label": "BaseFileHandler",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "class BaseFileHandler(metaclass=ABCMeta):\n    @abstractmethod\n    def load_from_fileobj(self, file, **kwargs):\n        pass\n    @abstractmethod\n    def dump_to_fileobj(self, obj, file, **kwargs):\n        pass\n    @abstractmethod\n    def dump_to_str(self, obj, **kwargs):\n        pass",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "JsonHandler",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "class JsonHandler(BaseFileHandler):\n    def load_from_fileobj(self, file):\n        return json.load(file)\n    def dump_to_fileobj(self, obj, file, **kwargs):\n        json.dump(obj, file, **kwargs)\n    def dump_to_str(self, obj, **kwargs):\n        return json.dumps(obj, **kwargs)\nclass PickleHandler(BaseFileHandler):\n    def load_from_fileobj(self, file, **kwargs):\n        return pickle.load(file, **kwargs)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "PickleHandler",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "class PickleHandler(BaseFileHandler):\n    def load_from_fileobj(self, file, **kwargs):\n        return pickle.load(file, **kwargs)\n    def load_from_path(self, filepath, **kwargs):\n        return super(PickleHandler, self).load_from_path(\n            filepath, mode='rb', **kwargs)\n    def dump_to_str(self, obj, **kwargs):\n        kwargs.setdefault('protocol', 2)\n        return pickle.dumps(obj, **kwargs)\n    def dump_to_fileobj(self, obj, file, **kwargs):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "YamlHandler",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "class YamlHandler(BaseFileHandler):\n    def load_from_fileobj(self, file, **kwargs):\n        kwargs.setdefault('Loader', Loader)\n        return yaml.load(file, **kwargs)\n    def dump_to_fileobj(self, obj, file, **kwargs):\n        kwargs.setdefault('Dumper', Dumper)\n        yaml.dump(obj, file, **kwargs)\n    def dump_to_str(self, obj, **kwargs):\n        kwargs.setdefault('Dumper', Dumper)\n        return yaml.dump(obj, **kwargs)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "is_str",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "def is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)\ndef slload(file, file_format=None, **kwargs):\n    \"\"\"Load data from json/yaml/pickle files.\n    This method provides a unified api for loading data from serialized files.\n    Args:\n        file (str or :obj:`Path` or file-like object): Filename or a file-like",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "slload",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "def slload(file, file_format=None, **kwargs):\n    \"\"\"Load data from json/yaml/pickle files.\n    This method provides a unified api for loading data from serialized files.\n    Args:\n        file (str or :obj:`Path` or file-like object): Filename or a file-like\n            object.\n        file_format (str, optional): If not specified, the file format will be\n            inferred from the file extension, otherwise use the specified one.\n            Currently supported formats include \"json\", \"yaml/yml\" and\n            \"pickle/pkl\".",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "sldump",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "def sldump(obj, file=None, file_format=None, **kwargs):\n    \"\"\"Dump data to json/yaml/pickle strings or files.\n    This method provides a unified api for dumping data as strings or to files,\n    and also supports custom arguments for each file format.\n    Args:\n        obj (any): The python object to be dumped.\n        file (str or :obj:`Path` or file-like object, optional): If not\n            specified, then the object is dump to a str, otherwise to a file\n            specified by the filename or file-like object.\n        file_format (str, optional): Same as :func:`load`.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "file_handlers",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "peekOfCode": "file_handlers = {\n    'json': JsonHandler(),\n    'yaml': YamlHandler(),\n    'yml': YamlHandler(),\n    'pickle': PickleHandler(),\n    'pkl': PickleHandler(),\n}\n# ===========================\n# load and dump\n# ===========================",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.slio",
        "documentation": {}
    },
    {
        "label": "TimeCounter",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "peekOfCode": "class TimeCounter:\n    def __init__(self) -> None:\n        pass\n    def clear(self):\n        self.timedict = {}\n        self.basetime = time.perf_counter()\n    def timeit(self, name):\n        nowtime = time.perf_counter() - self.basetime\n        self.timedict[name] = nowtime\n        self.basetime = time.perf_counter()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "documentation": {}
    },
    {
        "label": "TimeHolder",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "peekOfCode": "class TimeHolder:\n    def __init__(self) -> None:\n        self.timedict = {}\n    def update(self, _timedict: dict):\n        for k, v in _timedict.items():\n            if k not in self.timedict:\n                self.timedict[k] = AverageMeter(name=k, val_only=True)\n            self.timedict[k].update(val=v)\n    def final_res(self):\n        return {k: v.avg for k, v in self.timedict.items()}",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "peekOfCode": "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f', val_only=False):\n        self.name = name\n        self.fmt = fmt\n        self.val_only = val_only\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.time_counter",
        "documentation": {}
    },
    {
        "label": "CocoClassMapper",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class CocoClassMapper:\n    def __init__(self) -> None:\n        self.category_map_str = {\n            '1': 1,\n            '2': 2,\n            '3': 3,\n            '4': 4,\n            '5': 5,\n            '6': 6,\n            '7': 7,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "Embedder",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class Embedder:\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        self.create_embedding_fn()\n    def create_embedding_fn(self):\n        embed_fns = []\n        d = self.kwargs['input_dims']\n        out_dim = 0\n        if self.kwargs['include_input']:\n            embed_fns.append(lambda x: x)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "APOPMeter",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class APOPMeter:\n    def __init__(self) -> None:\n        self.tp = 0\n        self.fp = 0\n        self.tn = 0\n        self.fn = 0\n    def update(self, pred, gt):\n        \"\"\"\n        Input:\n            pred, gt: Tensor()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "NiceRepr",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class NiceRepr:\n    \"\"\"Inherit from this class and define ``__nice__`` to \"nicely\" print your\n    objects.\n    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function\n    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.\n    If the inheriting class has a ``__len__``, method then the default\n    ``__nice__`` method will return its length.\n    Example:\n        >>> class Foo(NiceRepr):\n        ...    def __nice__(self):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "ModelEma",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class ModelEma(torch.nn.Module):\n    def __init__(self, model, decay=0.9997, device=None):\n        super(ModelEma, self).__init__()\n        # make a copy of the model for accumulating moving average of weights\n        self.module = deepcopy(model)\n        self.module.eval()\n        # import ipdb; ipdb.set_trace()\n        self.decay = decay\n        self.device = device  # perform ema on different device from model if set\n        if self.device is not None:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "BestMetricSingle",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class BestMetricSingle:\n    def __init__(self, init_res=0.0, better='large') -> None:\n        self.init_res = init_res\n        self.best_res = init_res\n        self.best_ep = -1\n        self.better = better\n        assert better in ['large', 'small']\n    def isbetter(self, new_res, old_res):\n        if self.better == 'large':\n            return new_res > old_res",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "BestMetricHolder",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "class BestMetricHolder:\n    def __init__(self, init_res=0.0, better='large', use_ema=False) -> None:\n        self.best_all = BestMetricSingle(init_res, better)\n        self.use_ema = use_ema\n        if use_ema:\n            self.best_ema = BestMetricSingle(init_res, better)\n            self.best_regular = BestMetricSingle(init_res, better)\n    def update(self, new_res, epoch, is_ema=False):\n        \"\"\"\n        return if the results is the best.",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "slprint",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def slprint(x, name='x'):\n    if isinstance(x, (torch.Tensor, np.ndarray)):\n        print(f'{name}.shape:', x.shape)\n    elif isinstance(x, (tuple, list)):\n        print('type x:', type(x))\n        for i in range(min(10, len(x))):\n            slprint(x[i], f'{name}[{i}]')\n    elif isinstance(x, dict):\n        for k, v in x.items():\n            slprint(v, f'{name}[{k}]')",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "clean_state_dict",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def clean_state_dict(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        if k[:7] == 'module.':\n            k = k[7:]  # remove `module.`\n        new_state_dict[k] = v\n    return new_state_dict\ndef renorm(img: torch.FloatTensor,\n           mean=[0.485, 0.456, 0.406],\n           std=[0.229, 0.224, 0.225]) -> torch.FloatTensor:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "renorm",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def renorm(img: torch.FloatTensor,\n           mean=[0.485, 0.456, 0.406],\n           std=[0.229, 0.224, 0.225]) -> torch.FloatTensor:\n    # img: tensor(3,H,W) or tensor(B,3,H,W)\n    # return: same as img\n    assert img.dim() == 3 or img.dim(\n    ) == 4, 'img.dim() should be 3 or 4 but %d' % img.dim()\n    if img.dim() == 3:\n        assert img.size(0) == 3, 'img.size(0) shoule be 3 but \"%d\". (%s)' % (\n            img.size(0),",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "to_device",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def to_device(item, device):\n    if isinstance(item, torch.Tensor):\n        return item.to(device)\n    elif isinstance(item, list):\n        return [to_device(i, device) for i in item]\n    elif isinstance(item, dict):\n        return {k: to_device(v, device) for k, v in item.items()}\n    else:\n        raise NotImplementedError(\n            'Call Shilong if you use other containers! type: {}'.format(",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "get_gaussian_mean",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def get_gaussian_mean(x, axis, other_axis, softmax=True):\n    \"\"\"\n    Args:\n        x (float): Input images(BxCxHxW)\n        axis (int): The index for weighted mean\n        other_axis (int): The other index\n    Returns: weighted index for axis, BxC\n    \"\"\"\n    mat2line = torch.sum(x, axis=other_axis)\n    # mat2line = mat2line / mat2line.mean() * 10",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "get_expected_points_from_map",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def get_expected_points_from_map(hm, softmax=True):\n    \"\"\"get_gaussian_map_from_points\n        B,C,H,W -> B,N,2 float(0, 1) float(0, 1)\n        softargmax function\n    Args:\n        hm (float): Input images(BxCxHxW)\n    Returns:\n        weighted index for axis, BxCx2. float between 0 and 1.\n    \"\"\"\n    # hm = 10*hm",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "get_embedder",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def get_embedder(multires, i=0):\n    import torch.nn as nn\n    if i == -1:\n        return nn.Identity(), 3\n    embed_kwargs = {\n        'include_input': True,\n        'input_dims': 3,\n        'max_freq_log2': multires - 1,\n        'num_freqs': multires,\n        'log_sampling': True,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def inverse_sigmoid(x, eps=1e-5):\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)\n    x2 = (1 - x).clamp(min=eps)\n    return torch.log(x1 / x2)\ndef get_raw_dict(args):\n    \"\"\"\n    return the dicf contained in args.\n    e.g:\n        >>> with open(path, 'w') as f:",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "get_raw_dict",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def get_raw_dict(args):\n    \"\"\"\n    return the dicf contained in args.\n    e.g:\n        >>> with open(path, 'w') as f:\n                json.dump(get_raw_dict(args), f, indent=2)\n    \"\"\"\n    if isinstance(args, argparse.Namespace):\n        return vars(args)\n    elif isinstance(args, dict):",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "stat_tensors",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def stat_tensors(tensor):\n    assert tensor.dim() == 1\n    tensor_sm = tensor.softmax(0)\n    entropy = (tensor_sm * torch.log(tensor_sm + 1e-9)).sum()\n    return {\n        'max': tensor.max(),\n        'min': tensor.min(),\n        'mean': tensor.mean(),\n        'var': tensor.var(),\n        'std': tensor.var()**0.5,",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "ensure_rng",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def ensure_rng(rng=None):\n    \"\"\"Coerces input into a random number generator.\n    If the input is None, then a global random state is returned.\n    If the input is a numeric value, then that is used as a seed to construct a\n    random state. Otherwise the input is returned as-is.\n    Adapted from [1]_.\n    Args:\n        rng (int | numpy.random.RandomState | None):\n            if None, then defaults to the global rng. Otherwise this can be an\n            integer or a RandomState class",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "random_boxes",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def random_boxes(num=1, scale=1, rng=None):\n    \"\"\"Simple version of ``kwimage.Boxes.random``\n    Returns:\n        Tensor: shape (n, 4) in x1, y1, x2, y2 format.\n    References:\n        https://gitlab.kitware.com/computer-vision/kwimage/blob/master/kwimage/structs/boxes.py#L1390\n    Example:\n        >>> num = 3\n        >>> scale = 512\n        >>> rng = 0",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "targets_to",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def targets_to(targets: List[Dict[str, Any]], device):\n    \"\"\"Moves the target dicts to the given device.\"\"\"\n    excluded_keys = [\n        'questionId',\n        'tokens_positive',\n        'strings_positive',\n        'tokens',\n        'dataset_name',\n        'sentence_id',\n        'original_img_id',",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "get_phrases_from_posmap",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "peekOfCode": "def get_phrases_from_posmap(posmap: torch.BoolTensor, tokenized: Dict,\n                            tokenizer: AutoTokenizer):\n    assert isinstance(posmap, torch.Tensor), 'posmap must be torch.Tensor'\n    if posmap.dim() == 1:\n        non_zero_idx = posmap.nonzero(as_tuple=True)[0].tolist()\n        token_ids = [tokenized['input_ids'][i] for i in non_zero_idx]\n        return tokenizer.decode(token_ids)\n    else:\n        raise NotImplementedError('posmap must be 1-dim')",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.utils",
        "documentation": {}
    },
    {
        "label": "ColorMap",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "peekOfCode": "class ColorMap:\n    def __init__(self, basergb=[255, 255, 0]):\n        self.basergb = np.array(basergb)\n    def __call__(self, attnmap):\n        # attnmap: h, w. np.uint8.\n        # return: h, w, 4. np.uint8.\n        assert attnmap.dtype == np.uint8\n        h, w = attnmap.shape\n        res = self.basergb.copy()\n        res = res[None][None].repeat(h, 0).repeat(w, 1)  # h, w, 3",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "documentation": {}
    },
    {
        "label": "COCOVisualizer",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "peekOfCode": "class COCOVisualizer:\n    def __init__(self, coco=None, tokenlizer=None) -> None:\n        self.coco = coco\n    def visualize(self, img, tgt, caption=None, dpi=180, savedir='vis'):\n        \"\"\"\n        img: tensor(3, H, W)\n        tgt: make sure they are all on cpu.\n            must have items: 'image_id', 'boxes', 'size'\n        \"\"\"\n        plt.figure(dpi=dpi)",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "documentation": {}
    },
    {
        "label": "renorm",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "peekOfCode": "def renorm(img: torch.FloatTensor,\n           mean=[0.485, 0.456, 0.406],\n           std=[0.229, 0.224, 0.225]) -> torch.FloatTensor:\n    # img: tensor(3,H,W) or tensor(B,3,H,W)\n    # return: same as img\n    assert img.dim() == 3 or img.dim(\n    ) == 4, 'img.dim() should be 3 or 4 but %d' % img.dim()\n    if img.dim() == 3:\n        assert img.size(0) == 3, 'img.size(0) shoule be 3 but \"%d\". (%s)' % (\n            img.size(0),",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "documentation": {}
    },
    {
        "label": "rainbow_text",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "peekOfCode": "def rainbow_text(x, y, ls, lc, **kw):\n    \"\"\"\n    Take a list of strings ``ls`` and colors ``lc`` and place them next to each\n    other, with text ls[i] being shown in color lc[i].\n    This example shows how to do both vertical and horizontal text, and will\n    pass all keyword arguments to plt.text, so you can set the font size,\n    family, etc.\n    \"\"\"\n    t = plt.gca().transData\n    fig = plt.gcf()",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.visualizer",
        "documentation": {}
    },
    {
        "label": "create_positive_map_from_span",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "peekOfCode": "def create_positive_map_from_span(tokenized, token_span, max_text_len=256):\n    \"\"\"construct a map such that positive_map[i,j] = True iff box i is associated to token j\n    Input:\n        - tokenized:\n            - input_ids: Tensor[1, ntokens]\n            - attention_mask: Tensor[1, ntokens]\n        - token_span: list with length num_boxes.\n            - each item: [start_idx, end_idx]\n    \"\"\"\n    positive_map = torch.zeros((len(token_span), max_text_len),",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "documentation": {}
    },
    {
        "label": "build_captions_and_token_span",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "peekOfCode": "def build_captions_and_token_span(cat_list, force_lowercase):\n    \"\"\"\n    Return:\n        captions: str\n        cat2tokenspan: dict\n            {\n                'dog': [[0, 2]],\n                ...\n            }\n    \"\"\"",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "documentation": {}
    },
    {
        "label": "build_id2posspan_and_caption",
        "kind": 2,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "peekOfCode": "def build_id2posspan_and_caption(category_dict: dict):\n    \"\"\"Build id2pos_span and caption from category_dict\n    Args:\n        category_dict (dict): category_dict\n    \"\"\"\n    cat_list = [item['name'].lower() for item in category_dict]\n    id2catname = {item['id']: item['name'].lower() for item in category_dict}\n    caption, cat2posspan = build_captions_and_token_span(\n        cat_list, force_lowercase=True)\n    id2posspan = {",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.util.vl_utils",
        "documentation": {}
    },
    {
        "label": "__version__",
        "kind": 5,
        "importPath": "model.AI-ModelScope.GroundingDINO.groundingdino.version",
        "description": "model.AI-ModelScope.GroundingDINO.groundingdino.version",
        "peekOfCode": "__version__ = '0.1.0'",
        "detail": "model.AI-ModelScope.GroundingDINO.groundingdino.version",
        "documentation": {}
    },
    {
        "label": "GroundingdinoGenerationPipeline",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.ms_wrapper",
        "description": "model.AI-ModelScope.GroundingDINO.ms_wrapper",
        "peekOfCode": "class GroundingdinoGenerationPipeline(Pipeline):\n    def __init__(\n            self,\n            model: Union[Model, str],\n            *args,\n            **kwargs):\n        import sys\n        sys.path.append(model)\n        model = GroundingdinoGeneration(model, **kwargs) if isinstance(model, str) else model\n        super().__init__(model=model, **kwargs)",
        "detail": "model.AI-ModelScope.GroundingDINO.ms_wrapper",
        "documentation": {}
    },
    {
        "label": "GroundingdinoGeneration",
        "kind": 6,
        "importPath": "model.AI-ModelScope.GroundingDINO.ms_wrapper",
        "description": "model.AI-ModelScope.GroundingDINO.ms_wrapper",
        "peekOfCode": "class GroundingdinoGeneration(TorchModel):\n    def __init__(self, model_dir=None, *args, **kwargs):\n        super().__init__(model_dir, *args, **kwargs)\n        self.logger = get_logger()\n        self.model = load_model(\n        model_config_path = model_dir+\"/groundingdino/config/GroundingDINO_SwinT_OGC.py\",\n        model_checkpoint_path = model_dir+\"/groundingdino_swint_ogc.pth\")\n        self.model = self.model.eval()\n    def forward(self, inputs: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:\n        IMAGE_PATH = inputs.get('IMAGE_PATH', None)",
        "detail": "model.AI-ModelScope.GroundingDINO.ms_wrapper",
        "documentation": {}
    },
    {
        "label": "GroundingDINOBase",
        "kind": 6,
        "importPath": "dino_base",
        "description": "dino_base",
        "peekOfCode": "class GroundingDINOBase:\n    def __init__(\n        self,\n        model_path: str = \"model/GroundingDINOBase\",\n        default_prompt: str = \"screen\",\n        output_dir: str = \"output\",\n        box_threshold: float = 0.1,\n        text_threshold: float = 0.3,\n        min_box_area_ratio: int = 0.05,\n        device: str | None = None,",
        "detail": "dino_base",
        "documentation": {}
    },
    {
        "label": "load_local_image",
        "kind": 2,
        "importPath": "dino_base",
        "description": "dino_base",
        "peekOfCode": "def load_local_image(image_path: str | Path) -> Image.Image:\n    path = Path(image_path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Image not found: {path}\")\n    if path.suffix.lower() not in {\".png\", \".jpg\", \".jpeg\"}:\n        raise ValueError(\"Only PNG or JPG images are supported.\")\n    return Image.open(path).convert(\"RGB\")\ndef image_loader(\n    image_dir: str | Path,\n) -> typing.Generator[typing.Tuple[Path, Image.Image], None, None]:",
        "detail": "dino_base",
        "documentation": {}
    },
    {
        "label": "image_loader",
        "kind": 2,
        "importPath": "dino_base",
        "description": "dino_base",
        "peekOfCode": "def image_loader(\n    image_dir: str | Path,\n) -> typing.Generator[typing.Tuple[Path, Image.Image], None, None]:\n    path = Path(image_dir)\n    if not path.exists():\n        raise FileNotFoundError(f\"Image directory not found: {path}\")\n    if not path.is_dir():\n        raise ValueError(\"Provided path is not a directory.\")\n    suffixes = {\".png\", \".jpg\", \".jpeg\"}\n    image_files: typing.List[Path] = sorted(",
        "detail": "dino_base",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "dino_base",
        "description": "dino_base",
        "peekOfCode": "def main():\n    pipeline = GroundingDINOBase()\n    for file, image in image_loader(\"images\"):\n        pipeline.process_image(image)\n        pipeline.crop_save_image(image, file.stem)\n        output_file = pipeline.save_image(image, file.stem)\n        print(f\"Processed image saved to: {output_file}\")\nif __name__ == \"__main__\":\n    main()",
        "detail": "dino_base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "dino_base",
        "description": "dino_base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass GroundingDINOBase:\n    def __init__(\n        self,\n        model_path: str = \"model/GroundingDINOBase\",\n        default_prompt: str = \"screen\",\n        output_dir: str = \"output\",\n        box_threshold: float = 0.1,\n        text_threshold: float = 0.3,\n        min_box_area_ratio: int = 0.05,",
        "detail": "dino_base",
        "documentation": {}
    },
    {
        "label": "download_model_dino_base",
        "kind": 2,
        "importPath": "download_model",
        "description": "download_model",
        "peekOfCode": "def download_model_dino_base():\n    model_id = \"muse/grounding-dino-base\"\n    model_dir = \"model/GroundingDINOBase\"\n    snapshot_download(\n        model_id,\n        local_dir=model_dir,\n    )\ndef download_model_idea_dino():\n    model_id = \"AI-ModelScope/GroundingDINO\"\n    model_dir = \"model/AI-ModelScope/GroundingDINO\"",
        "detail": "download_model",
        "documentation": {}
    },
    {
        "label": "download_model_idea_dino",
        "kind": 2,
        "importPath": "download_model",
        "description": "download_model",
        "peekOfCode": "def download_model_idea_dino():\n    model_id = \"AI-ModelScope/GroundingDINO\"\n    model_dir = \"model/AI-ModelScope/GroundingDINO\"\n    snapshot_download(\n        model_id,\n        local_dir=model_dir,\n    )\nif __name__ == \"__main__\":\n    # download_model_dino_base()\n    download_model_idea_dino()",
        "detail": "download_model",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "test_torch",
        "description": "test_torch",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")",
        "detail": "test_torch",
        "documentation": {}
    }
]